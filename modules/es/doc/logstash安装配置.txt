------------------------------------logstash安装配置---------------------------------
logstash 主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。
一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作在一并发往elasticsearch上。
logstash可以同时作为agent和server来从指定的位置（如file，mysql, redis）抽取数据，并进行文档化，然后发送给es，也可以只作为服务端，配合轻量化的filebeat抽取数据。
logstash事件处理有三个阶段：inputs → filters → outputs。是一个接收，处理，转发日志的工具。
支持系统日志，webserver日志，错误日志，应用日志，总之包括所有可以抛出来的日志类型。
inputs中一些常用的输入:
    file：从文件系统的文件中读取，类似于tial -f命令
    syslog：在514端口上监听系统日志消息，并根据RFC3164标准进行解析
    redis：从redis service中读取
    beats：从filebeat中读取
filters中一些常用的过滤器为：
    grok：解析任意文本数据，Grok 是 Logstash 最重要的插件。
    它的主要作用就是将文本格式的字符串，转换成为具体的结构化的数据，配合正则表达式使用。
    官方提供的grok表达式：https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns
    grok在线调试：https://grokdebug.herokuapp.com/

    mutate：对字段进行转换。例如对字段进行删除、替换、修改、重命名等。
    drop：丢弃一部分events不进行处理。
    clone：拷贝 event，这个过程中也可以添加或移除字段。
    geoip：添加地理信息(为前台kibana图形化展示使用)
outputs是logstash处理管道的最末端组件。一个event可以在处理过程中经过多重输出，但是一旦所有的outputs都执行结束，这个event也就完成生命周期。
一些常见的outputs为：
    elasticsearch：可以高效的保存数据，并且能够方便和简单的进行查询。
    file：将event数据保存到文件中。
    graphite：将event数据发送到图形化组件中，一个很流行的开源存储图形化展示的组件。
    codecs：codecs 是基于数据流的过滤器，它可以作为input，output的一部分配置。codecs可以轻松的分割发送过来已经被序列化的数据。
一些常见的codecs：
    json：使用json格式对数据进行编码/解码。
    multiline：将汇多个事件中数据汇总为一个单一的行。比如：java异常信息和堆栈信息

wget https://artifacts.elastic.co/downloads/logstash/logstash-6.3.0.tar.gz
tar -zxvf logstash-6.3.0.tar.gz
cd logstash-6.3.0
vim ./config/logstash.yml                                       ###设置logstash配置，一般默认设置即可
    node.name: xxx                                              ###设置节点名称，默认为主机名
    path.data: /data/setup/logstash-6.3.0/data                  ###指定logstash 和插件使用的持久化目录,默认即可
    config.reload.automatic: true                               ###开启配置文件自动加载
    config.reload.interval: 3                                   ###定义配置文件重载时间周期
    http.host: "0.0.0.0"                                        ###设置监听ip,默认值是127.0.0.1这个本地ip，本地ip无法远程通信

vim ./config/logstash.conf                                      ###定义数据源，这里配置logstash 从Filebeat 输入、过滤、输出至elasticsearch
    input {
            beats {
                port => 5044
            }
            //file {
            //        path => "/data/log/logstash/*/*"
            //        start_position => "beginning"               ###从文件开始处读写
            //}
    }
    filter {                                                    ###定义数据的格式
      grok {
        match => { "message" => "%{DATA:invokeTime} \[%{DATA:threadName}\] %{DATA:logLevel} %{DATA:invokeMethod} - %{GREEDYDATA:content}"}
      }
      date {                                                    ####定义时间戳的格式
        match => [ "invokeTime", "yyyy-MM-dd HH:mm:ss" ]
        locale => "cn"
        target => "invokeTime"
      }
    }
    output {                                                    ###将输出保存到elasticsearch
        elasticsearch {
            hosts => "localhost:9200"
            index => "app_log_%{+YYYY-MM}"
        }
    }

./bin/logstash -f ../config/logstash.conf --config.test_and_exit       ###检查配置文件是否正确，--config.test_and_exit 表示检测后就退出
./bin/lagstash -f ../config/logstash.conf                              ###启动logstash
./bin/lagstash -f ../config/logstash.conf --config.reload.automatic    ###启动并自动检测配置文件的变动和自动重新加载配置文件,
默认,检测配置文件的间隔时间是3秒,可以通过参数--config.reload.interval <second> 来设置。
注:stdin标准输入插件不支持自动重启,syslog作为输入源,当重载配置文件时,会崩溃。
ps aux | grep logstash                                           ###查看logstash进程
netstat -anp | grep LISTEN |grep 9600                            ###查看logstash对应的端口是否开启监听
./bin/logstash-plugin list --verbose                             ###查询已安装的logstash插件
./bin/logstash-plugin install logstash-input-jdbc                ###安装插件logstash-input-jdbc
./bin/logstash-plugin remove logstash-input-jdbc                 ###移除插件logstash-input-jdbc
./bin/logstash-plugin update logstash-input-jdbc                 ###升级插件logstash-input-jdbc
-------------------------------------------------------------------------------------