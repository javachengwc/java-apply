---------------------------------------es性能优化--------------------------------
es写入优化
a,内存参数配置不合理，如果内存足,可配置30G每个es数据实例节点。
b,bulk提交量过大，导致内存被堆满，一次提交的bulk数量不宜过大，实践证明5-10MB左右大小合适。
c,客户端IP，端口配置问题,es的客户端采用轮询方式，所以尽量配置所有节点的IP、端口，或者开启嗅探功能。
d,写入时指定DOC ID，导致读IO高。写入时指定DOC ID，意味着首先需要判断ID是否重复，
在大数据量下，可能会需要从磁盘进行一次读操作，从而占用大量的磁盘IO，导致写入速度慢。
e,bulk队列积压，请求线程被拒绝。大量的bulk队列被等待或者积压，导致线程被拒绝，这时需要适当降低业务请求的并发量。
f,热分片问题,单个索引的分片集中分布在某几个机器节点上，导致写入压力无法均匀地分布到各个机器节点上，形成阻塞的问题。
g,集群不稳定，大量分片迁移和恢复,如果有大量的分片在做均衡迁移或者恢复，都会占用大量的资源，导致写入资源被占用。
h,部分实例长时间不断的full gc，导致实例处于假死状态。但此时并没有完全脱离集群，写入请求仍然往这个节点发送，此时节点已经无法处理了。需要正确的方式重启节点服务。
i,索引段合并太频繁同样会占用大量的IO资源，如果不是SSD盘，将索引段合并线程设置为1。
j,集群分片数过多，过多分片导致es内部写扩大。可降低主分片数，维护单副本保证数据冗余即可，对于部分超大索引，可考虑采用0副本的策略。
k,索引设计方面，id自动生成(特别适合时序类型索引，以时间为轴，按天索引，数据只增加，不更新)，
去掉打分机制，去掉DocValues策略，嵌套对象类型调整为Object对象类型。以通过减少索引字段，降低Indexing Thread线程的IO压力。
l,根据es官方提供的优化手段进行调整，包括refresh，fsync，flush时间，index_buffer_size等

es查询优化
a,内存参数配置不合理，文件系统缓存不足,需要预留一定的内存给lucene文件缓存使用
b,查询范围过大，一次查询过多的分片，如全表扫描查询
c,进行深度翻页查询或查询返回的结果集过大，建议使用scroll查询分批次返回
d,查询语句不是最优，如过滤查询可以使用filter，不需要评分，减少很多计算
e,用text字段进行排序，造成fielddata占用大量的内存,排序使用keyword字段
f,es查询避免使用脚本，在使用日期范围检索时，避免使用now的查询，它不走缓存
f,索引段文件过多，需要定时的进行索引段合并
g,分片分布不均衡，未能充分利用机器资源,尽量让分片均匀分片，查询的时候才能充分发挥分布式的优势
h,索引数据结构mapping设计不合理，如不需要分词的keyword,分词器设计不合理，如存在过度分词
i,索引分片过大，如单个分片达到100g,建议单个分片50g以内
j,为只读索引执行force-merge,将Lucene索引合并为单个分段，可以提升查询速度。
k,预热文件系统cache,如果es重启，文件系统缓存为空，此时搜索会比较慢。可以使用index.store.preload设置，通过指定文件扩展名，让系统将对应文件加载到内存中。
l,调节搜索请求中的batched_reduce_size,默认情况下，聚合操作在协调节 点需要等所有的分片都取回结果后才执行，
  使用 batched_reduce_size 参数可以不等待全部分片返回结果，而是在指定数量的分片返回结果之后 就可以先处理一部分（reduce）。
  这样可以避免协调节点在等待全部结果的过程中占用大量内存，避免极端情况下可能导致的OOM。该字段默认值为512，从es5.4开始支持。
m,利用自适应副本选择（ARS）提升es响应速度,es的ARS实现基于这样一个设定：对每个搜索请求，将分片的每个副本进行排序，
  以确定哪个最可能是转发请求的“最佳”副本。与轮询方式向分片的每个副本发送请求不同，es选择“最佳”副本并将请求路由到那里。
1，优化首选项--filesystem cache
os cache，操作系统的缓存，往es里写的数据，实际上都写到磁盘文件里去了，磁盘文件里的数据操作系统会自动将里面的数据缓存到os cache中。
es搜索依赖于底层的filesystem cache，如果给filesystem cache更多的内存，尽量让内存可以容纳所有的index segment file索引数据文件，那么搜索时基本是走内存，性能会非常高。
比如，3节点es集群，每节点内存64G，总内存，64 * 3 = 192g，每几点给es jvm heap 32G，那么剩下留给filesystem cache的就是32g，集群总共filesystem cache就是32 * 3 = 96g。
如果索引数据文件，在3节点上一共占用了1T的磁盘容量，es数据量是1t，每节点大概300g，但集群总的filesystem cache的内存不到100g，1/10的数据可以放内存，其他的都在磁盘，
执行搜索操作，大部分操作都是走磁盘，性能比较差。所以要让es性能好，最佳的情况下，就是机器的内存，至少可以容纳总数据量的一半。
在es中就存少量的数据，就是用来搜索的那些索引，尽量在es里，就存储必须用来搜索的数据,内存留给filesystem cache，这样可以更好满足数据几乎全部走内存来搜索。
2，数据预热
热搜索的数据可以提前每隔一段时间，提前访问进行搜索一次，刷到filesystem cache里去，后面实际搜索查看这些热数据，就可直接从内存里搜索了。
3，冷热分离
es可以做类似于mysql的水平拆分，将大量的访问很少，频率很低的数据，写一个索引，将访问很频繁的热数据单独写一个索引。
这样可以确保热数据在被预热之后，尽量都让他们留在filesystem os cache里，别让冷数据给冲刷掉。
4，document模型设计
es里面的复杂的关联查询，复杂的查询语法，一般性能都不太好，所以对es索引结构的设计，在一开始就设计好。
写入es的数据就已经是组装关联好的数据，搜索时，不需要利用es的搜索语法去完成类似join之类的搜索。
对于es的复杂查询操作，比如join，nested，parent-child搜索尽量避免，性能比较差。
尽量避免使用nested或 parent/child，能不用就不用；nested query慢， parent/child query 更慢，比nested query慢上百倍；
因此能在mapping设计阶段搞定的（大宽表设计或采用比较smart的数据结构），就不要用父子关系的mapping。
避免使用动态值作字段(key),  动态递增的mapping，会导致集群崩溃；同样，也需要控制字段的数量，业务中不使用的字段，就不要索引。
5,避免使用es分页查询，改用scroll api滚动查询
es的分页是会把每个shard上存储的前pageSize*pageNo条数据都查到一个协调节点上，协调节点对这些数据进行一些合并、处理，再获取到最终页数据。
就是分布式的分页查询,在往后翻页查询中，翻的越深，每个shard返回的数据越多，协调节点处理的时间越长。越翻到后面越是慢。
所以索引尽量避免深度分页,使用scroll api来进行获取翻页数据。scroll会一次性生成所有数据的一个快照，然后每次翻页就是通过游标移动，性能比分页高很多。
唯一缺点是，只能一页一页往后翻，不能随意跳到任何页。同时这个scroll是保留一段时间内的数据快照，有一定的时效性。
---------------------------------------------------------------------------------
es调优
hot_threads（GET /_nodes/hot_threads&interval=30）
抓取30s的节点上占用资源的热线程，并通过排查占用资源最多的TOP线程来判断对应的资源消耗是否正常，
一般情况下，bulk，search类的线程占用资源都可能是业务造成的，
但是如果是merge线程占用了大量的资源，就应该考虑是不是创建index或者刷磁盘间隔太小，批量写入size太小造成的。

pending_tasks（GET /_cluster/pending_tasks）
一些任务只能由主节点去处理，比如创建一个新的索引或者在集群中移动分片，由于一个集群中只能有一个主节点，所以只有这一master节点可以处理集群级别的元数据变动。
如果元数据变动的次数比主节点能处理的还快，这会导致等待中的操作会累积成队列。这个时候可以通过pending_tasks api分析当前什么操作阻塞了es的队列，
比如，集群异常时，会有大量的shard在recovery，如果集群在大量创建新字段，会出现大量的put_mappings的操作，所以正常情况下，需要禁用动态mapping。

es节点优化
1,适当增大写入buffer和bulk队列长度，提高写入性能和稳定性
vim conf/elasticsearch.yml
    indices.memory.index_buffer_size: 15%
    thread_pool.bulk.queue_size: 2048
2,在规模比较大的集群中，防止新建shard时扫描所有shard的元数据，提升shard分配速度
    cluster.routing.allocation.disk.include_relocations: false
3,设置内存熔断参数，防止写入或查询压力过高导致OOM
    indices.breaker.total.limit: 30%
    indices.breaker.request.limit: 6%
    indices.breaker.fielddata.limit: 3%
4,调整查询使用的cache，避免cache占用过多的jvm内存
    indices.queries.cache.count: 500
    indices.queries.cache.size: 5%

es使用优化
1,不需要索引source的情况下，可以关闭source,有update操作的index不能关闭source
PUT index1
{
    "mappings": {
        "index_type": {
            "_source": {
                "enabled": false
            }
        }
    }
}
2,不需要docvalues的字段，关闭docvalues
PUT index1
{
    "mappings": {
        "index_type": {
            "properties": {
                "session_id": {
                    "type": "keyword",
                    "doc_values": false
                }
            }
        }
    }
}
3,es索引中all字段是一个特殊的字段，它是所有字段值拼接成一个字符串后，做分词，保存倒排索引，用于支持整个doc的全文检索。
若无此需求，可关闭all字段
PUT index1
{
    "mapping": {
        "index_type": {
            "_all": {
                "enabled": false
            }
        }
    }
}
4,开启最佳压缩,对于_source字段，可设置压缩算法，提高数据压缩率
PUT /index1/_settings
{
    "index.codec": "best_compression"
}
5,写入数据时可使用bulk接口批量写入，提高写入效率。每个bulk请求的doc数量设定区间推荐为1k~1w。
6,调整translog刷盘策略，通过命令调整 translog 持久化策略为异步周期性执行，并适当调整translog的刷盘周期。
PUT index1
{
    "settings": {
        "index": {
           "translog": {
              "sync_interval": "5s",
              "durability": "async"
           }
        }
    }
}
7,调整refresh_interval,es须通过refresh的过程把内存中的数据转换成Lucene的完整segment后，才可以被搜索。
es默认1s写入的数据可以被查询到，会产生大量的segment，检索性能会受到影响。非实时的场景可以调大，以降低系统开销。
8,merge并发控制，es的一个index由多个shard组成，而一个shard其实就是一个lucene的index，它又由多个segment组成，
且Lucene会不断地把一些小的segment合并成一个大的segment，这个过程被称为段merge。
index.merge.scheduler.max_thread_count控制并发的merge线程数,
如果存储是并发性能较好的SSD，可以用系统默认的max(1, min(4, availableProcessors / 2))，
PUT /index1/_settings
{
    "index.merge.scheduler.max_thread_count": 2
}
9,索引是类似增量日志记录的数据，写入数据时id自生成。
如果写入数据时显示指定id，es会先发起查询来确定index中是否已经有相同id的doc存在，
若有则先删除原有doc再写入新doc。这样每次写入时，es都会耗费一定的资源做查询
10,适当使用routing，es会将routing相同的数据写入到同一个分片中（也可以是多个，由index.routingpartitionsize参数控制）。
如果查询时指定routing，那么es只会查询routing指向的那个分片，可显著降低调度开销，提升查询效率。
11,一些场景下，使用query-bool-filter组合取代普通query
默认情况下，es通过一定的算法计算返回的每条数据与查询语句的相关度，并通过score字段来表征。
对于非全文索引的使用场景，用户并不care查询结果与查询条件的相关度，只是想精确的查找目标数据。
此时，可以通过query-bool-filter组合来让es不计算score，并且尽可能的缓存filter的结果集，供后续包含相同filter的查询使用，提高查询效率。
//普通查询
POST index1/_search
{
    "query": {
        "term": {
            "user": "aa"
        }
    }
}
// query-bool-filter查询
POST index1/_search
{
    "query": {
        "bool": {
            "filter": {
                "term": {
                    "user": "aa"
                }
            }
        }
    }
}
12,分片数和副本数按需控制
13,Segment Memory优化
es底层采用lucene做存储，而Lucene的一个index又由若干segment组成，每个segment都会建立自己的倒排索引用于数据查询。
lucene为了加速查询，为每个segment的倒排做了一层前缀索引，这个索引在lucene4.0以后采用的数据结构是FST (Finite State Transducer)。
lucene加载segment的时候将其全量装载到内存中，加快查询速度。这部分内存被称为SegmentMemory， 常驻内存，占用heap，无法被GC。
当jvm内存有限，可想办法降低SegmentMemory的使用量，常用方法有下面几个：
    a,定期删除不使用的index
    b,对于不常访问的index，可以通过close接口将其关闭，用到时再打开
    c,通过force_merge接口强制合并segment，降低segment数量
14,控制索引的大小，数据量过大的索引，都会带来一定的读写性能开销，
可通过某种方式做分割，写入es的数据存入不同的index,比如按照日期滚动生成index。
15,禁止动态mapping，动态mapping可能造成集群元数据一直变更，导致集群不稳定,业务不可控。
---------------------------------------------------------------------------------