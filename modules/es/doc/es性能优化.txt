---------------------------------------es性能优化--------------------------------
es写入优化
a,内存参数配置不合理，如果内存足,可配置30G每个es数据实例节点。
b,bulk提交量过大，导致内存被堆满，一次提交的bulk数量不宜过大，实践证明5-10MB左右大小合适。
c,客户端IP，端口配置问题,es的客户端采用轮询方式，所以尽量配置所有节点的IP、端口，或者开启嗅探功能。
d,写入时指定DOC ID，导致读IO高。写入时指定DOC ID，意味着首先需要判断ID是否重复，
在大数据量下，可能会需要从磁盘进行一次读操作，从而占用大量的磁盘IO，导致写入速度慢。
e,bulk队列积压，请求线程被拒绝。大量的bulk队列被等待或者积压，导致线程被拒绝，这时需要适当降低业务请求的并发量。
f,热分片问题,单个索引的分片集中分布在某几个机器节点上，导致写入压力无法均匀地分布到各个机器节点上，形成阻塞的问题。
g,集群不稳定，大量分片迁移和恢复,如果有大量的分片在做均衡迁移或者恢复，都会占用大量的资源，导致写入资源被占用。
h,部分实例长时间不断的full gc，导致实例处于假死状态。但此时并没有完全脱离集群，写入请求仍然往这个节点发送，此时节点已经无法处理了。需要正确的方式重启节点服务。
i,索引段合并太频繁同样会占用大量的IO资源，如果不是SSD盘，将索引段合并线程设置为1。
j,集群分片数过多，过多分片导致es内部写扩大。可降低主分片数，维护单副本保证数据冗余即可，对于部分超大索引，可考虑采用0副本的策略。
k,索引设计方面，id自动生成(特别适合时序类型索引，以时间为轴，按天索引，数据只增加，不更新)，
去掉打分机制，去掉DocValues策略，嵌套对象类型调整为Object对象类型。以通过减少索引字段，降低Indexing Thread线程的IO压力。
l,根据es官方提供的优化手段进行调整，包括refresh，fsync，flush时间，index_buffer_size等

es查询优化
a,内存参数配置不合理，文件系统缓存不足,需要预留一定的内存给lucene文件缓存使用
b,查询范围过大，一次查询过多的分片，如全表扫描查询
c,进行深度翻页查询或查询返回的结果集过大，建议使用scroll查询分批次返回
d,查询语句不是最优，如过滤查询可以使用filter，不需要评分，减少很多计算
e,用text字段进行排序，造成fielddata占用大量的内存,排序使用keyword字段
f,es查询避免使用脚本，在使用日期范围检索时，避免使用now的查询，它不走缓存
f,索引段文件过多，需要定时的进行索引段合并
g,分片分布不均衡，未能充分利用机器资源,尽量让分片均匀分片，查询的时候才能充分发挥分布式的优势
h,索引数据结构mapping设计不合理，如不需要分词的keyword,分词器设计不合理，如存在过度分词
i,索引分片过大，如单个分片达到100g,建议单个分片50g以内
j,为只读索引执行force-merge,将Lucene索引合并为单个分段，可以提升查询速度。
k,预热文件系统cache,如果es重启，文件系统缓存为空，此时搜索会比较慢。可以使用index.store.preload设置，通过指定文件扩展名，让系统将对应文件加载到内存中。
l,调节搜索请求中的batched_reduce_size,默认情况下，聚合操作在协调节 点需要等所有的分片都取回结果后才执行，
  使用 batched_reduce_size 参数可以不等待全部分片返回结果，而是在指定数量的分片返回结果之后 就可以先处理一部分（reduce）。
  这样可以避免协调节点在等待全部结果的过程中占用大量内存，避免极端情况下可能导致的OOM。该字段默认值为512，从es5.4开始支持。
m,利用自适应副本选择（ARS）提升es响应速度,es的ARS实现基于这样一个设定：对每个搜索请求，将分片的每个副本进行排序，
  以确定哪个最可能是转发请求的“最佳”副本。与轮询方式向分片的每个副本发送请求不同，es选择“最佳”副本并将请求路由到那里。
1，优化首选项--filesystem cache
os cache，操作系统的缓存，往es里写的数据，实际上都写到磁盘文件里去了，磁盘文件里的数据操作系统会自动将里面的数据缓存到os cache中。
es搜索依赖于底层的filesystem cache，如果给filesystem cache更多的内存，尽量让内存可以容纳所有的index segment file索引数据文件，那么搜索时基本是走内存，性能会非常高。
比如，3节点es集群，每节点内存64G，总内存，64 * 3 = 192g，每几点给es jvm heap 32G，那么剩下留给filesystem cache的就是32g，集群总共filesystem cache就是32 * 3 = 96g。
如果索引数据文件，在3节点上一共占用了1T的磁盘容量，es数据量是1t，每节点大概300g，但集群总的filesystem cache的内存不到100g，1/10的数据可以放内存，其他的都在磁盘，
执行搜索操作，大部分操作都是走磁盘，性能比较差。所以要让es性能好，最佳的情况下，就是机器的内存，至少可以容纳总数据量的一半。
在es中就存少量的数据，就是用来搜索的那些索引，尽量在es里，就存储必须用来搜索的数据,内存留给filesystem cache，这样可以更好满足数据几乎全部走内存来搜索。
2，数据预热
热搜索的数据可以提前每隔一段时间，提前访问进行搜索一次，刷到filesystem cache里去，后面实际搜索查看这些热数据，就可直接从内存里搜索了。
3，冷热分离
es可以做类似于mysql的水平拆分，将大量的访问很少，频率很低的数据，写一个索引，将访问很频繁的热数据单独写一个索引。
这样可以确保热数据在被预热之后，尽量都让他们留在filesystem os cache里，别让冷数据给冲刷掉。
4，document模型设计
es里面的复杂的关联查询，复杂的查询语法，一般性能都不太好，所以对es索引结构的设计，在一开始就设计好。
写入es的数据就已经是组装关联好的数据，搜索时，不需要利用es的搜索语法去完成类似join之类的搜索。
对于es的复杂查询操作，比如join，nested，parent-child搜索尽量避免，性能比较差。
尽量避免使用nested或 parent/child，能不用就不用；nested query慢， parent/child query 更慢，比nested query慢上百倍；
因此能在mapping设计阶段搞定的（大宽表设计或采用比较smart的数据结构），就不要用父子关系的mapping。
避免使用动态值作字段(key),  动态递增的mapping，会导致集群崩溃；同样，也需要控制字段的数量，业务中不使用的字段，就不要索引。
5,避免使用es分页查询，改用scroll api滚动查询
es的分页是会把每个shard上存储的前pageSize*pageNo条数据都查到一个协调节点上，协调节点对这些数据进行一些合并、处理，再获取到最终页数据。
就是分布式的分页查询,在往后翻页查询中，翻的越深，每个shard返回的数据越多，协调节点处理的时间越长。越翻到后面越是慢。
所以索引尽量避免深度分页,使用scroll api来进行获取翻页数据。scroll会一次性生成所有数据的一个快照，然后每次翻页就是通过游标移动，性能比分页高很多。
唯一缺点是，只能一页一页往后翻，不能随意跳到任何页。同时这个scroll是保留一段时间内的数据快照，有一定的时效性。
---------------------------------------------------------------------------------
es调优
hot_threads（GET /_nodes/hot_threads&interval=30）
抓取30s的节点上占用资源的热线程，并通过排查占用资源最多的TOP线程来判断对应的资源消耗是否正常，
一般情况下，bulk，search类的线程占用资源都可能是业务造成的，
但是如果是merge线程占用了大量的资源，就应该考虑是不是创建index或者刷磁盘间隔太小，批量写入size太小造成的。

pending_tasks（GET /_cluster/pending_tasks）
一些任务只能由主节点去处理，比如创建一个新的索引或者在集群中移动分片，由于一个集群中只能有一个主节点，所以只有这一master节点可以处理集群级别的元数据变动。
如果元数据变动的次数比主节点能处理的还快，这会导致等待中的操作会累积成队列。这个时候可以通过pending_tasks api分析当前什么操作阻塞了es的队列，
比如，集群异常时，会有大量的shard在recovery，如果集群在大量创建新字段，会出现大量的put_mappings的操作，所以正常情况下，需要禁用动态mapping。






---------------------------------------------------------------------------------