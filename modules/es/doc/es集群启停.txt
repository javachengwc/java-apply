----------------------------------es集群------------------------------
es集群关闭
1，禁止分片自动分布
curl -XPUT localhost:9200/_cluster/settings -d '{
    "persistent": {
        "cluster.routing.allocation.enable": "none"
    }
}'
2，执行同步刷新     curl -XPOST localhost:9200/_flush/synced
3，各节点逐个关闭   kill pid

es集群启动，一般启动后到集群为正常(green)状态需要5-10分钟
1，逐个启动节点     ./elasticsearch -d
2，等待所有节点加入集群，查看集群状态  http://localhost:9200/_cat/health?v
3，启用分片自动分布
curl -XPUT localhost:9200/_cluster/settings -d '{
    "persistent": {
        "cluster.routing.allocation.enable": "all"
    }
}'
4，等待集群可用     http://localhost:9200/_cat/health?v
                   http://localhost:9200/_cat/recovery?v

es集群启动过程，选主节点-->gateway选举元信息过程-->allocation分片分配过程-->recovery分片恢复过程
1,选主节点
a,参选人数需要过半，达到配置节点数（discovery.zen.minimum_master_nodes）后就选出了临时的主。
  为什么是临时的？每个节点运行排序取最大值的算法，结果不一定相同。
b,得票数须过半。某节点被选为主，须判断加入他的节点数过半，确认 master 身份。
2,gateway过程
选出的 Master 和集群元信息的新旧程度没有关系。因此它的首要任务是选举元信息，让各节点把其各自存储的元信息发过来，
根据版本号确定最新的元信息，然后把这个信息广播下去，这样，集群的所有节点都有了最新的元信息。
为了集群一致性，参与选举的元信息数量需要过半，Master 发布集群信息是的成功规则也是过半。
在选举过程中，不接受新节点的加入请求。集群元信息选举完毕后，Master 发布首次集群状态，然后开始选举 shard 级元信息。
3,allocation过程
选举 shard 级元信息，构建路由表，是在 allocation 模块完成的。初始阶段，所有的 shard 都处于UNASSIGNED状态。此时，首先要做的是分配主分片。
a,选主分片,所有的分配工作都是 Master 来做的，此时，Master 不知道主分片在哪，它向集群的所有节点获取分片的元信息。
然后，Master 等待所有的请求返回，正常情况下就有了这个 shard 的信息，然后根据某种策略选一个分片做主。
现在有了 shard的多份信息，取决于副本数设置了多少。现在考虑把谁做主分片。
es5以下的版本，通过对比shard 级元信息的版本号来决定。但在多副本的情况下，如果只有一个shard 信息汇报上来，那它一定会被选为主分片，
但也许数据不是最新的，版本号比它大的那个 shard 所在节点还没启动。为了解决这个问题，es5开始实施一种新的策略：给每个 shard 都设置一个 UUID，
然后在集群级的元信息记录，哪个 shard 是最新的，因为 es 是先写主分片，再由主分片节点转发请求去写副分片，所以主分片所在节点肯定是最新的，
所以，es5中，主分片是通过集群级元信息中记录的最新主分片的列表来确定主分片的。
如果集群设置 cluster.routing.allocation.enable: none ,只要节点可分配主分片，会强制分配主分片。
因此，在设置了上述选项的情况下，集群重启后的状态为 yellow，而非 red。
b,先副分片，主分片选举完成后，从上一个过程汇总的 shard 信息中选择一个副本做副分片（如果是配置一个副本的情况下）。
也可以延迟分配副分片,通过index.unassigned.node_left.delayed_timeout设置。
4,recovery过程
es的recovery代表的是数据恢复或者叫做数据重新分布。当有节点加入或退出时.
它会根据机器的负载对索引分片进行重新分配，当挂掉的节点再次重新启动的时候也会进行数据恢复。
如果某个主分片在，而复制分片所在的节点挂掉了，那么master需要另行选择一个可用节点，将这个主分片的复制分片分配到可用节点上，然后进行主从分片的数据复制。
如果某个主分片所在的节点挂掉了，复制分片还在，那么master会主导将复制分片升级为主分片，然后再做主从分片数据复制。
如果某个分片的主副分片都挂掉了，则暂时无法恢复，而是要等持有相关数据的节点重新加入集群后，master才能主持数据恢复相关操作。
a,主分片 recovery，未刷盘数据从 translog 恢复，每次刷盘完毕，translog 都会被清空，
因此把 translog 中的数据全部重放，建立 lucene 索引，如此完成主分片的 recovery。
b,副分片 recovery，副分片需要恢复成与主分片一致，同时，恢复期间允许新的索引操作。
因此主分片节点，通过调用 lucene 接口把 shard 做快照。副分片把 主分片shard 数据拷贝到副本节点。
这期间主分片translog 会做快照，这个快照里包含从拷贝开始的所有新增索引。副分片把此translog快照数据进行重放。
这样副分片恢复完成，主副两 shard 就有相同的 syncid 且 doc 数相同。

----------------------------------------------------------------------