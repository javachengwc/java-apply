-------------------------------------mongodb复制集----------------------------------
mongodb复制集就是一组mongod实例掌管同一个数据集，实例可以在不同的机器上面。
实例中包含一个主库，接受客户端所有的写入操作，其他都是副本实例，从主库上获得数据并保持同步。
主服务器包含了所有的改变操作（写）的日志。副本服务器集群包含有所有的主服务器数据，
当主服务器挂掉了，就会在副本服务器上重新选取一个成为主服务器。
有的复制集还有一个仲裁者，仲裁者不存储数据，只是负责通过心跳包来确认集群中集合的数量，
并在主服务器选举的时候作为仲裁决定结果。每个副本集中可以有多个仲裁者。
当节点数目为奇数时，可以不需要仲裁节点。
当节点数目为偶数个时，需要部署一个仲裁节点，否则偶数个节点，当主节点挂了后，其他节点会变为只读。
mongodb复制集的基本架构:
a,三个存储数据的成员的复制集
      一个主库,两个从库组成，主库宕机时，这两个从库都可以被选为主库。
      当主库宕机后,两个从库都会进行竞选，其中一个变为主库，当原主库恢复后，作为从库加入当前的复制集群即可。
b,三个成员的复制集中,存在arbiter节点
      在三个成员的复制集中，一个主库,一个从库，可以在选举中成为主库，一个aribiter节点，在选举中，只进行投票，不能成为主库
mongodb复制集通过replSetInitiate命令（或mongo shell的rs.initiate()）进行初始化，
初始化后各个成员间开始发送心跳消息，并发起Priamry选举操作，
获得大多数成员投票支持的节点，会成为Primary，其余节点成为Secondary。

mongodb复制集中成员
Primary     主库,接受客户端所有的写入操作,包含了所有的改变操作（写）的日志。
Secondary   从库,参与Primary选举（自身也可能会被选为Primary），
            并从Primary同步最新写入的数据，以保证与Primary存储相同的数据。
Arbiter     Arbiter节点只参与投票，不能被选为Primary，并且不从Primary同步数据。
            比如部署了一个2个节点的复制集，1个Primary，1个Secondary，
            任意节点宕机，复制集将不能提供服务了（无法选出Primary），
            这时可以给复制集添加一个Arbiter节点，即使有节点宕机，仍能选出Primary。
            当复制集成员为偶数时，最好加入一个Arbiter节点，以提升复制集可用性。
Priority0   Priority0节点的选举优先级为0，不会被选举为Primary
            比如跨机房A、B部署了一个复制集，并且想指定Primary必须在A机房，
            这时可以将B机房的复制集成员Priority设置为0，这样Primary就一定会是A机房的成员。
            最好将大多数节点部署在A机房，否则网络分区时可能无法选出Primary
Vote0       Mongodb 3.0里，复制集成员最多50个，参与Primary选举投票的成员最多7个，
            其他成员（Vote0）的vote属性必须设置为0，即不参与投票。
Hidden      隐藏节点,Hidden节点不能被选为主（Priority为0），客户端将不会把读请求分发到隐藏节点上，即使设定了复制集读选项 。
            因隐藏节点将不会收到来自应用程序的请求，可使用Hidden节点做一些数据备份、离线计算的任务，不会影响复制集的服务。
Delayed     延时节点,延时节点的数据集是延时的,Delayed节点必须是Hidden节点，并且其数据落后与Primary一段时间（可配置，比如1个小时）。
            因Delayed节点的数据比Primary落后一段时间，当错误或者无效的数据写入Primary时，可通过Delayed节点的数据来恢复到之前的时间点。
mongodb默认是由主节点读写数据的，副本节点上不允许读，需要设置slaveOk=true使副本节点可以读。
在交互命令行执行db.getMongo().setSlaveOk()或rs.slaveOk()也能使节点可读,但是下次再通过mongo进入实例的时候，查询仍会报错
如果是通过java访问secondary的则会报下面的异常com.mongodb.MongoException: not talking to master and retries used up
解决办法如下:
第1种方法-->在java代码中调用dbFactory.getDb().slaveOk();
第2种方法-->在java代码中调用
    dbFactory.getDb().setReadPreference(ReadPreference.secondaryPreferred());//在复制集中优先读secondary，如果secondary访问不了的时候就从master中读
    或
    dbFactory.getDb().setReadPreference(ReadPreference.secondary());//只从secondary中读，如果secondary访问不了的时候就不能进行查询
第3种方法：在配置mongo的时候增加slave-ok="true"也支持直接从secondary中读
<mongo:mongo id="mongo" host="${mongodb.host}" port="${mongodb.port}">
        <mongo:options slave-ok="true"/>
</mongo:mongo>

复制集节点选举机制
    1、自身是否能够与主节点连通；
    2、希望被选为主节点的备份节点的数据是否最新；
    3、有没有其他更高优先级的成员可以被选举为主节点；
    4、如果被选举为主节点的成员能够得到副本集中“大多数”成员的投票，则它会成为主节点，
    5、希望成为主节点的成员必须使用复制将自己的数据更新为最新；

复制集节点数据初始化过程
    1、首先做一些记录前的准备工作：选择一个成员作为同步源，在local.me集合中为自己创建一个标识符，
       删除索引已存在的数据库，以一个全新的状态开始进行同步；该过程中，所有的数据都会被删除。
    2、然后克隆，就是将同步源的所有记录全部复制到本地。
    3、然后就进入oplog同步的第一步，克隆过程中的所有操作都会被记录到oplog中。
    4、接下来就是oplog同步过程的第二步，用于将第一个oplog同步中的操作记录下来。
    5、截止当前，本地的数据应该与主节点在某个时间点的数据集完全一致了，可以开始创建索引了。
    6、若当前节点的数据仍然落后同步源，那么oplog同步过程的最后一步就是将创建索引期间的所有操作全部同步过出来。
    7、现在当前成员已经完成了初始化数据的同步，切换到普通状态，这时该节点就可以成为备份节点了。

mongodb复制集同步，参见图mongodb-sync.png
Primary上的写入会记录oplog，存储到一个固定大小的capped collection里，Secondary主动从Primary上拉取oplog并重放应用到自身，以保持数据与Primary节点上一致。
新节点加入（或者主动向Secondary发送resync）时，Secondary会先进行一次initial sync，即全量同步，
遍历Primary上的所有DB的所有集合，将数据拷贝到自身节点，然后读取在全量同步这个时间段产生的oplog并重放。
全量同步结束后，Secondary就开始从结束时间点建立tailable cursor，不断的从同步源拉取oplog并重放应用到自身，
这个过程并不是由一个线程来完成的，mongodb为了提升同步效率，将拉取oplog以及重放oplog分到了不同的线程来执行。
producer thread，此线程不断的从同步源上拉取oplog，并加入到一个BlockQueue的队列里保存着，BlockQueue最大存储240MB的oplog数据，
    当超过这个阈值时，就必须等到oplog被replBatcher消费掉才能继续拉取。
replBatcher thread，此线程负责逐个从producer thread的队列里取出oplog，并放到自己维护的队列里，这个队列最多允许5000个元素，并且元素总大小不超过512MB，
    当队列满了时，就需要等待oplogApplication消费掉。
oplogApplication会取出replBatch thread当前队列的所有元素，并将元素根据docId（如果存储引擎不支持文档锁，则根据集合名称）分散到不同的replWriter线程，
replWriter线程将所有的oplog应用到自身；等待所有oplog都应用完毕，oplogApplication线程将所有的oplog顺序写入到local.oplog.rs集合。
producer的buffer和apply线程的统计信息都可以通过db.serverStatus().metrics.repl来查询到。
默认情况下，Secondary采用16个replWriter线程来重放oplog，可通过启动时设置replWriterThreadCount参数来定制线程数，
在启动时指定mongod --setParameter replWriterThreadCount=32 或
在配置文件中指定
setParameter:
    replWriterThreadCount: 32
如果因Primary上的写入qps很高，经常出现Secondary同步无法追上的问题，可以考虑以下处理
    保证Primary节点有充足的服务能力，如果用户的请求就能把Primary的资源跑得很满，那么势必会影响到主备同步。
    配置更高的replWriterThreadCount，Secondary上加速oplog重放，代价是更高的内存开销
    合理配置oplog的大小，可以结合写入的情况，预估oplog的大小，比如oplog能存储一天的写入量，
    这样即使备同步慢、故障、或者临时下线维护等，只要不超过1天，恢复后还是有希望继续同步的。
    阿里云MongoDB数据库增加了patch，能做到在线修改oplog的大小。
    将writeOpsToOplog步骤分散到多个replWriter线程来并发执行，这个是官方目前在考虑的策略之一，参考Secondaries unable to keep up with primary under WiredTiger
在测试过程中，向Primary模拟约10000 qps的写入，观察Secondary上的同步，写入速率小于Primary，大致只有3000左右的qps，
同时观察到producer的buffer很快就达到饱和，可以判断出oplog重放的线程跟不上。
当提升线程数到32时，同步的情况大大改观，主备写入的qps基本持平，主备上数据同步的延时控制在1s以内。
出现Secondary同步延迟的情况也有可能是因oplog的写入被放大，导致同步追不上
mongodb用于同步的oplog具有一个重要的幂等性，也就是说，一条oplog在备上重放多次，得到的结果跟重放一次结果是一样的，
这个特性简化了同步的实现，Secondary不需要有专门的逻辑去保证一条oplog在备上必须仅能重放一次。
为了保证幂等性，记录oplog时，通常需要对写入的请求做一下转换，举个例子，某文档x字段当前值为100，用户向Primary发送一条{$inc: {x: 1}}，
记录oplog时会转化为一条{$set: {x: 101}的操作，才能保证幂等性。
简单元素的操作，$inc 转化为 $set并没有什么影响，执行开销上也差不多，但当遇到数组元素操作时，情况就不一样了。
在数组尾部push 2个元素，查看oplog发现$push操作被转换为了$set操作（设置数组指定位置的元素为某个值）。
则当向数组的头部添加元素时，oplog里的$set操作不再是设置数组某个位置的值（因为基本所有的元素位置都调整了），而是$set数组最终的结果，
即整个数组的内容都要写入oplog。当push操作指定了$slice或者$sort参数时，oplog的记录方式也是一样的，会将整个数组的内容作为$set的参数。
当数组非常大时，对数组的一个小更新，可能就需要把整个数组的内容记录到oplog里，Secondary同步时会拉取oplog并重放，
Primary到Secondary同步oplog的流量是客户端到Primary网络流量的上百倍，导致主备间网卡流量跑满，而且由于oplog的量太大，
旧的内容很快被删除掉，最终导致Secondary追不上，转换为RECOVERING状态。
MongoDB对json的操作支持很强大，尤其是对数组的支持，但在文档里使用数组时，一定得注意上述问题，避免数组的更新导致同步开销被无限放大的问题。
使用数组时，尽量注意数组的元素个数不要太多，总的大小也不要太大。尽量避免对数组进行更新操作。
如果一定要更新，尽量只在尾部插入元素，复杂的逻辑可以考虑在业务层面上来支持。
------------------------------------------------------------------------------------